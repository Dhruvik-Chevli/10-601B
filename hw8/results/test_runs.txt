python value_iteration.py ../data/maze_2.txt ../results/value_output_2.txt ../results/q_value_output_2.txt ../results/policy_output_2.txt 20 0.9

python value_iteration.py ../data/maze_1.txt ../results/value_output_1.txt ../results/q_value_output_1.txt ../results/policy_output_1.txt 20 0.9

python value_iteration.py ../data/medium_maze.txt ../results/value_output_med.txt ../results/q_value_output_med.txt ../results/policy_output_med.txt 20 0.9

python environment.py ../data/maze_2.txt ../results/large_maze_output.feedback ../data/large_maze_action_seq.txt

python q_learning.py ../data/tiny_maze.txt ../results/q_learning/value_output.txt ../results/q_learning/q_value_output.txt ../results/q_learning/policy_output.txt 1000 20 0.8 0.9 0.05

python q_learning.py ../data/maze_1.txt ../results/q_learning/value_output_1.txt ../results/q_learning/q_value_output_1.txt ../results/q_learning/policy_output_1.txt 5000 50 0.8 0.9 0.05

python q_learning.py ../data/maze_2.txt ../results/q_learning/value_output_2.txt ../results/q_learning/q_value_output_2.txt ../results/q_learning/policy_output_2.txt 5000 100 0.8 0.9 0.05

python q_learning.py ../data/medium_maze.txt ../results/q_learning/value_output_med.txt ../results/q_learning/q_value_output_med.txt ../results/q_learning/policy_output_med.txt 5000 50 0.8 0.9 0.05

python value_iteration-emperical.py ../data/maze_2.txt ../results/value_output_2.txt ../results/q_value_output_2.txt ../results/policy_output_2.txt 20 0.9

python value_iteration-emperical.py ../data/maze_1.txt ../results/value_output_1.txt ../results/q_value_output_1.txt ../results/policy_output_1.txt 20 0.9



python q_learning.py ../data/maze_1.txt ../results/q_learning/value_output_1.txt ../results/q_learning/q_value_output_1.txt ../results/q_learning/policy_output_1.txt 2000 50 0.1 0.9 0.2

python q_learning-emperical.py ../data/maze_1.txt ../results/q_learning/value_output_1.txt ../results/q_learning/q_value_output_1.txt ../results/q_learning/policy_output_1.txt 2000 50 0.1 0.9 0.01


python q_learning-emperical.py ../data/maze_2.txt ../results/q_learning/value_output_2.txt ../results/q_learning/q_value_output_2.txt ../results/q_learning/policy_output_2.txt 2000 100 0.1 0.9 0.01

python q_learning-emperical.py ../data/maze_2.txt ../results/q_learning/value_output_2.txt ../results/q_learning/q_value_output_2.txt ../results/q_learning/policy_output_2.txt 2000 200 0.1 0.9 0.8

python q_learning-emperical.py ../data/maze_2.txt ../results/q_learning/value_output_2.txt ../results/q_learning/q_value_output_2.txt ../results/q_learning/policy_output_2.txt 2000 50 0.1 0.9 0.2

python q_learning-emperical.py ../data/maze_1.txt ../results/q_learning/value_output_1.txt ../results/q_learning/q_value_output_1.txt ../results/q_learning/policy_output_1.txt 2000 50 0.1 0.9 0.2



python value_iteration-emperical.py ../data/theory-maze.txt ../results/value_theory.txt ../results/q_theory.txt ../results/policy_theory.txt 20 1
